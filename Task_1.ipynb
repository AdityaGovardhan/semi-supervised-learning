{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Text for saving the notebook\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.pipeline import Pipeline\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from scipy.sparse import coo_matrix, vstack, hstack\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "twenty_train = fetch_20newsgroups(subset='train', shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# count_vect = CountVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# count_vect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train_counts = count_vect.fit_transform(twenty_train.data)\n",
    "# X_train_counts.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11314, 130107)"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "X_train_tfidf = tfidf_vectorizer.fit_transform(twenty_train.data)\n",
    "X_train_tfidf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = MultinomialNB(alpha = 0.01).fit(X_train_tfidf, twenty_train.target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7532, 130107)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.8352363250132767"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "twenty_test = fetch_20newsgroups(subset='test', shuffle=True)\n",
    "# X_test_counts = count_vect.fit_transform(twenty_test.data)\n",
    "X_test_tfidf = tfidf_vectorizer.transform(twenty_test.data)\n",
    "print(X_test_tfidf.shape)\n",
    "predicted = clf.predict(X_test_tfidf)\n",
    "predicted\n",
    "\n",
    "# predicted = text_clf.predict(twenty_test.data)\n",
    "np.sum(predicted == twenty_test.target)/len(predicted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SECOND PART STARTS FROM HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Step-1\n",
    "#Splitting the train data into labelled and unlabelled\n",
    "# labelled_X, unlabelled_X, labelled_y, _ = train_test_split(twenty_train.data[:4000], twenty_train.target[:4000], train_size = 0.2)\n",
    "\n",
    "# clf.class_log_prior_.shape, clf.feature_log_prob_.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Finding the TF-IDF from the whole data for labelled and unlabelled data (Feature extraction step)\n",
    "# tfidf_vectorizer = TfidfVectorizer()\n",
    "# tfidf_vectorizer_obj = tfidf_vectorizer.fit(twenty_train.data)\n",
    "# labelled_X_tfidf = tfidf_vectorizer.transform(labelled_X)\n",
    "# print(labelled_X_tfidf.shape)\n",
    "# unlabelled_X_tfidf = tfidf_vectorizer.transform(unlabelled_X)\n",
    "# print(unlabelled_X_tfidf.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def calculate_log_likelihood(class_log_prob, feature_log_prob):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# X_train.shape, X_test.shape, y_train.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_label.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Actual Initialization step where we are fitting a classifier on ONLY labelled data\n",
    "def perform_EM(labelled_X_tfidf, labelled_y, unlabelled_X_tfidf):\n",
    "    multinomial_clf = MultinomialNB(alpha = 0.01) #Defacto ALWAYS use this\n",
    "    multinomial_clf.fit(labelled_X_tfidf, labelled_y)\n",
    "    class_prob = multinomial_clf.class_log_prior_\n",
    "    feature_prob = multinomial_clf.feature_log_prob_\n",
    "    old_loss = 0\n",
    "    loss = -1*(calculate_unlabeled_loss(class_prob, feature_prob, unlabelled_X_tfidf) + calculate_labelled_loss(class_prob, feature_prob, labelled_X_tfidf, labelled_y))\n",
    "    diff = (loss - old_loss)\n",
    "    print(\"diff before\",diff)\n",
    "    threshold = np.exp(-10)\n",
    "    while(abs(diff) > threshold):\n",
    "        old_loss = loss\n",
    "        #E-step\n",
    "        pred_labels = multinomial_clf.predict(unlabelled_X_tfidf)\n",
    "\n",
    "        #Combining the data\n",
    "        X_total = vstack([labelled_X_tfidf, unlabelled_X_tfidf])\n",
    "        Y_total = np.concatenate((labelled_y, pred_labels), axis = 0)\n",
    "        X_total.shape, Y_total.shape\n",
    "\n",
    "\n",
    "        #M-step\n",
    "        multinomial_clf.fit(X_total, Y_total)\n",
    "        \n",
    "        class_prob = multinomial_clf.class_log_prior_\n",
    "        feature_prob = multinomial_clf.feature_log_prob_\n",
    "        print(type(feature_prob), feature_prob.shape)\n",
    "        loss = -1*( calculate_unlabeled_loss(class_prob, feature_prob, unlabelled_X_tfidf) + calculate_labelled_loss(class_prob, feature_prob, labelled_X_tfidf, labelled_y))\n",
    "\n",
    "        diff = old_loss - loss\n",
    "        print(\"diff\", diff)\n",
    "    return multinomial_clf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#After the EM step calculate the loss and converge based on it.\n",
    "#Once that is done then use the trained classifier and predict for the test samples and caculate the accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for x in X_total[:1]:\n",
    "#     print(x)\n",
    "# X_total[:1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Testing the word-document one hot representation:\n",
    "small_documents = twenty_train.data[:100]\n",
    "# print(small_documents[:2])\n",
    "vectorizer = CountVectorizer(binary = True)\n",
    "doc_term_csr_matrix = vectorizer.fit_transform(small_documents)\n",
    "#This is a numpy ndarray now\n",
    "doc_term_numpy_matrix = doc_term_csr_matrix.toarray()\n",
    "# print(vectorizer.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_term_numpy_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savetxt(\"foo.csv\", mat.toarray(), delimiter=\",\", fmt = \"%d\")\n",
    "# X_label[:10].toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer1 = CountVectorizer(binary=True)\n",
    "cv_obj = vectorizer1.fit(twenty_train.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mat = cv_obj.transform(small_documents)\n",
    "# type(mat)\n",
    "mat.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(tfidf_vectorizer.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_unlabeled_loss(class_prob, feature_prob, unlabelled_data_tfidf):\n",
    "    \"\"\"\n",
    "    unlabeled_data_one_hot_matrix: Assuming this to be ndarray\n",
    "    \"\"\"\n",
    "    print(\"unlabelled_data shape\", unlabelled_data_tfidf.shape)\n",
    "    n = unlabelled_data_tfidf.shape[0]\n",
    "    \n",
    "    p_xi_cj = np.sum(feature_prob, axis = 1, keepdims = True)\n",
    "    class_prob = class_prob.reshape(class_prob.shape[0],1)\n",
    "    print(class_prob.shape)\n",
    "    sum_loss = class_prob + p_xi_cj\n",
    "    sum_for_all_classes = np.sum(sum_loss, axis = 0, keepdims = True)\n",
    "    sum_for_all_classes = sum_for_all_classes[0]\n",
    "    total_loss = n*sum_for_all_classes\n",
    "    return total_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_labelled_loss(class_prob, feature_prob, labelled_data_tfidf, labelled_labels):\n",
    "    n_labelled = labelled_data_tfidf.shape[0]\n",
    "    loss = 0\n",
    "    sum_of_feature_log_probs = np.sum(feature_prob, axis = 1, keepdims = True)\n",
    "    for i in range(n_labelled):\n",
    "        c_index = labelled_labels[i]\n",
    "        prob_for_c = class_prob[c_index]\n",
    "        feature_value_for_c = sum_of_feature_log_probs[c_index]\n",
    "        loss += (prob_for_c + feature_value_for_c)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def calculate_unlabeled_loss(class_prob, feature_prob, unlabeled_data_one_hot_matrix):\n",
    "#     \"\"\"\n",
    "#     unlabeled_data_one_hot_matrix: Assuming this to be ndarray\n",
    "#     \"\"\"\n",
    "#     class_doc_matrix = np.matmul(feature_prob, unlabeled_data_one_hot_matrix.T)\n",
    "#     print(\"class_doc_matrix \", class_doc_matrix.shape)\n",
    "#     #this is the sum loss matrix for all documents and all classes\n",
    "    \n",
    "#     class_prob = class_prob.reshape(class_prob.shape[0],1)\n",
    "#     print(class_prob.shape)\n",
    "#     sum_loss = class_prob + class_doc_matrix\n",
    "#     sum_for_all_classes = np.sum(sum_loss, axis = 0, keepdims = True)\n",
    "#     total_loss = np.sum(sum_for_all_classes, axis = 1)\n",
    "#     total_loss = total_loss[0]\n",
    "#     print(\"unlabeled loss\", total_loss)\n",
    "#     return total_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def calculate_labelled_loss(class_prob, feature_prob, labeled_data_one_hot_matrix, labelled_labels):\n",
    "#     n_labelled = labeled_data_one_hot_matrix.shape[0]\n",
    "#     loss = 0\n",
    "#     for i in range(n_labelled):\n",
    "#         c_index = labelled_labels[i]\n",
    "#         prob_for_c = class_prob[c_index]\n",
    "#         feature_vector_for_c = feature_prob[c_index]\n",
    "#         word_doc_belonging_vector = labeled_data_one_hot_matrix[c_index]\n",
    "#         log_p_x_i_c_j = np.matmul(feature_vector_for_c, word_doc_belonging_vector)\n",
    "#         loss += (prob_for_c + log_p_x_i_c_j)\n",
    "#     print(\"labeled loss\", loss)\n",
    "#     return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Using the new classifier generated from the maximization step\n",
    "feat = clf_new.feature_log_prob_\n",
    "print(feat.shape)\n",
    "class_prob = clf_new.class_log_prior_\n",
    "unlabeled_data_one_hot_matrix = cv_obj.transform(unlabelled_X)\n",
    "unlabeled_data_one_hot_matrix = unlabeled_data_one_hot_matrix.toarray()\n",
    "prod = np.matmul(feat, unlabeled_data_one_hot_matrix.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# type(class_prob), type(unlabeled_data_one_hot_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class_prob.reshape(class_prob.shape[0], 1).shape, prod.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feat[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unlabeled_data_one_hot_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2262, 130107)\n",
      "(9052, 130107)\n"
     ]
    }
   ],
   "source": [
    "#Calling EM and testing the prediction\n",
    "#These steps needs to be done if pasting in a function or main in .py file\n",
    "#Step - 1\n",
    "labelled_X, unlabelled_X, labelled_y, _ = train_test_split(twenty_train.data, twenty_train.target, train_size = 0.2)\n",
    "# vectorizer1 = CountVectorizer(binary=True)\n",
    "# cv_obj = vectorizer1.fit(twenty_train.data)\n",
    "# Finding the TF-IDF from the whole data for labelled and unlabelled data (Feature extraction step)\n",
    "#Step-2\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "tfidf_vectorizer.fit(twenty_train.data)\n",
    "labelled_X_tfidf = tfidf_vectorizer.transform(labelled_X)\n",
    "print(labelled_X_tfidf.shape)\n",
    "unlabelled_X_tfidf = tfidf_vectorizer.transform(unlabelled_X)\n",
    "print(unlabelled_X_tfidf.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "unlabelled_data shape (9052, 130107)\n",
      "(20, 1)\n",
      "diff before [2.92128488e+11]\n",
      "<class 'numpy.ndarray'> (20, 130107)\n",
      "unlabelled_data shape (9052, 130107)\n",
      "(20, 1)\n",
      "diff [-1.94134149e+10]\n",
      "<class 'numpy.ndarray'> (20, 130107)\n",
      "unlabelled_data shape (9052, 130107)\n",
      "(20, 1)\n",
      "diff [-20081958.41949463]\n",
      "<class 'numpy.ndarray'> (20, 130107)\n",
      "unlabelled_data shape (9052, 130107)\n",
      "(20, 1)\n",
      "diff [-2199613.47625732]\n",
      "<class 'numpy.ndarray'> (20, 130107)\n",
      "unlabelled_data shape (9052, 130107)\n",
      "(20, 1)\n",
      "diff [0.]\n"
     ]
    }
   ],
   "source": [
    "em_trained_clf = perform_EM(labelled_X_tfidf, labelled_y, unlabelled_X_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7798725438130643"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#TESTING\n",
    "twenty_test = fetch_20newsgroups(subset='test', shuffle=True)\n",
    "X_test_tfidf = tfidf_vectorizer_obj.transform(twenty_test.data)\n",
    "predicted = em_trained_clf.predict(X_test_tfidf)\n",
    "\n",
    "# print(accuracy_score(twenty_test.target, predicted))\n",
    "np.sum(predicted == twenty_test.target)/len(predicted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import reuters\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "cats = reuters.categories()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_docs = len(reuters.paras())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_multi = 0\n",
    "for c in cats:\n",
    "    lcat = len(reuters.paras(categories=[c]))\n",
    "    total_multi += lcat"
   ]
  }
 ],
 "metadata": {
  "file_extension": ".py",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
